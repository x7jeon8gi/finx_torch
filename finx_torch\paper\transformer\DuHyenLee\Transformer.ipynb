{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+uEfEBkGbpPse9UUJLhsp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/x7jeon8gi/finx_torch/blob/master/finx_torch%5Cpaper%5Ctransformer%5CDuHyenLee%5CTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "aJn_W-sE6kGD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HVq37sLM4a-w"
      },
      "outputs": [],
      "source": [
        "## 전체적으로 클래스들을 정리했습니다.\n",
        "\n",
        "class Transformer(nn.Module): # Transformer class 생성 / nn.module 상속받음\n",
        "  def __init__(self,Encoder,Decoder): # 함수 생성자 재정의\n",
        "    super().__init__() #__init__의 모든 코드 받음\n",
        "\n",
        "    self.Encoder =  Encoder\n",
        "    self.Decoder = Decoder\n",
        "\n",
        "  def __call__(self,Enc_input,Dec_input):\n",
        "    self.Enc_input = Enc_input\n",
        "    self.Dec_input = Dec_input\n",
        "    self.d_model = self.Enc_input.shape[2] # (3차원 데이터를 받았다고 가정한다.)\n",
        "    self.Linear = nn.Linear(self.d_model , self.d_model)\n",
        "    \n",
        "  def forward(self,Enc_input,Dec_input):\n",
        "    self.Encoder_context = Encoder(Enc_input)\n",
        "    self.Dec_output = Decoder(Enc_input , Dec_input)\n",
        "    self.net_output = self.Linear(Dec_output)\n",
        "    return nn.softmax(self.net_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module): # n개의 Encoder_Layer를 가짐\n",
        "  def __init__(self,Num_of_Enc_Layers):\n",
        "    super().__init__()\n",
        "    self.Num_of_Enc_Layers = Num_of_Enc_Layers\n",
        "    self.Encoder_Layer =  Encoder_Layer\n",
        "  def forward(self,Enc_input):\n",
        "    Enc_hs = Enc_input\n",
        "    \n",
        "    for _ in range(len(self.Num_of_Enc_Layers)):\n",
        "      Enc_hs = self.Encoder_Layer(Enc_hs)\n",
        "    \n",
        "    return Enc_hs"
      ],
      "metadata": {
        "id": "zVY8qujJAKvo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_Layer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.Embedding = Embedding\n",
        "    self.Multi_head_attention = Multi_head_attention\n",
        "    self.Layer_normal = Layer_normal\n",
        "    self.Position_Wise_Feed_forward = Position_Wise_Feed_forward\n",
        "    self.Enc_input_data = Enc_input_data\n",
        "\n",
        "  def forward(self, Enc_input_data):\n",
        "    \n",
        "    Embedded_Enc = self.Embedding(Enc_input_data)\n",
        "    Self_Attention_Enc = self.Multi_head_attention(Embedded_Enc)\n",
        "    Sub_1_output = self.Layer_normal(Embedded_Enc , Self_Attention_Enc)\n",
        "    FFN_output = self.Position_Wise_Feed_forward(Sub_1_output)\n",
        "    Sub_2_output = self.Layer_normal(Sub_1_output , FFN_output)\n",
        "\n",
        "    return Sub_2_output"
      ],
      "metadata": {
        "id": "_TC2r_FkC5O7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module): # n개의 Encoder_Layer를 가짐\n",
        "  def __init__(self,Num_of_Dec_Layers):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.Deccoder_Layer = Decoder_Layer\n",
        "    self.Num_of_Dec_Layers = Num_of_Dec_Layers\n",
        "    self.Dec_layers = nn.ModuleList([Decoder_Layer() for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self , Enc_output , Dec_input):\n",
        "        Dec_input = \n",
        "      for layer in self.Dec_layers\n",
        "        \n",
        "        Dec_result = layer(Dec_input , Enc_output)\n",
        "\n",
        "      return Dec_hs"
      ],
      "metadata": {
        "id": "qNX4uhOBAlNf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([1,2,3,4,5]) "
      ],
      "metadata": {
        "id": "zq40QCmS3UGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6386b8-f25d-45c0-b0b8-cf3ace0e1894"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_Layer(nn.Module):\n",
        "  def __init__(self,Decoder_value):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.Embedding = Embedding\n",
        "    self.Multi_head_attention = Multi_head_attention\n",
        "    self.Layer_normal = Layer_normal\n",
        "    self.Position_Wise_Feed_forward = Position_Wise_Feed_forward\n",
        "    self.Cross_attention = Cross_attention\n",
        "\n",
        "  def forward(self,Dec_input,Enc_output):\n",
        "    \n",
        "    Embedded_Dec = self.Embedding(Dec_input)\n",
        "    Masked_Dec = self.Multi_head_attention(Embedded_Dec , Masking = True)\n",
        "    Sub_1_output = self.Layer_normal(Embedded_Dec , Masked_Dec)\n",
        "    \n",
        "    Cross_val = Cross_attention(Enc_output,Sub_1_output)\n",
        "    Sub_2_output = self.Layer_normal(Cross_val , Sub_1_output)\n",
        "    \n",
        "    FFN_output = self.Position_Wise_Feed_forward(Sub_2_output)\n",
        "    Sub_3_output = self.Layer_normal(Sub_2_output , FFN_output)\n",
        "\n",
        "    return Sub_3_output"
      ],
      "metadata": {
        "id": "ZgfGJJ4sC-hz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  \n",
        "  def __call__(self,input_data):\n",
        "    self.input_data = input_data\n",
        "    # d_model : 한개의 원핫인코딩 vector의 길이\n",
        "    # n_seq : 토큰의 갯수\n",
        "    # n_seq * d_model : input_data\n",
        "    self.n_seq , self.d_model =  self.input_data.shape[0],self.input_data.shape[1]    \n",
        "\n",
        "  def forward(self, input_data):\n",
        "    \n",
        "    Positional_Enc = np.zeros((self.n_seq, self.d_model))\n",
        "\n",
        "    for pos in range(self.n_seq):\n",
        "        for i in range(d_model//2):\n",
        "            self.Positional_Enc[pos, 2*i]   = np.sin( pos / (10000**(2*i/d_model)) )\n",
        "            self.Positional_Enc[pos, 2*i+1] = np.cos( pos / (10000**(2*i/d_model)) )\n",
        "  \n",
        "    return input_data + Positional_Enc"
      ],
      "metadata": {
        "id": "LGhY45UmCNtc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi_head_attention(nn.Module):\n",
        "  \n",
        "  def __init__(self,num_heads):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads \n",
        "\n",
        "     \n",
        "  # 마스킹을 위한 함수 // 출처 : https://paul-hyun.github.io/transformer-02/\n",
        "  def get_attn_decoder_mask(self,input_data , num_heads):\n",
        "    self.n_seq = input_data.shape[1]\n",
        "    self.subsequent_mask = torch.ones_like(input_data).unsqueeze(-1).expand(num_heads, self.n_seq, self.n_seq)\n",
        "    self.subsequent_mask_full = subsequent_mask_tr.triu(diagonal=1)* np.exp(-10) + subsequent_mask_tr.tril(diagonal=0) # upper triangular part of a matrix(2-D)\n",
        "    return self.subsequent_mask_full\n",
        "    # 여러번의 scaled_dot_product할 필요 없이, 한번에 행렬로 연산하면 된다.\n",
        "    \n",
        "  def __call__(self , input_data ,Masking = False): \n",
        "    \n",
        "    self.input_data = input_data\n",
        "    self.Masking = Masking\n",
        "    self.d_model = self.input_data.shape[2] # tensor # n_seq, d_model\n",
        "    self.d_k = self.d_model / self.num_heads  \n",
        "    \n",
        "    # query / key / value 생성하는 FFN\n",
        "    self.FFN_Q = nn.Linear(self.d_model,self.d_model)\n",
        "    self.FFN_K = nn.Linear(self.d_model,self.d_model)\n",
        "    self.FFN_V = nn.Linear(self.d_model,self.d_model) # d_v는 다른 값이어도 상관없다.\n",
        "    self.FFN_O = nn.Linear(self.d_model,self.d_model)\n",
        "\n",
        "    if self.Masking == True:\n",
        "  \n",
        "      def forward(self,input_data):\n",
        "    \n",
        "        Q = self.FFN_Q(input_data)\n",
        "        K = self.FFN_K(input_data)\n",
        "        V = self.FFN_V(input_data)\n",
        "    \n",
        "        Q = Q.view(self.num_heads,-1,self.d_k)\n",
        "        K = K.view(self.num_heads,-1,self.d_k)\n",
        "        V = V.view(self.num_heads,-1,self.d_k)\n",
        "  \n",
        "        scaled_E = torch.matmul(Q,K.transpose(-2,-1))/np.sqrt(self.d_k)\n",
        "            \n",
        "        mask = self.get_attn_decoder_mask(scaled_E , self.num_heads)\n",
        "        masked_E = torch.mul(scaled_E , mask)\n",
        "        soft_E = F.softmax(masked_E)\n",
        "        attention = torch.matmul(soft_E , V)\n",
        "        result = FFN_O(attention)\n",
        "        return result\n",
        "\n",
        "    else:\n",
        "      def forward(self,input_data):\n",
        "    \n",
        "        Q = self.FFN_Q(input_data)\n",
        "        K = self.FFN_K(input_data)\n",
        "        V = self.FFN_V(input_data)\n",
        "    \n",
        "        Q = Q.view(self.num_heads,-1,self.d_k)\n",
        "        K = K.view(self.num_heads,-1,self.d_k)\n",
        "        V = V.view(self.num_heads,-1,self.d_k)\n",
        "\n",
        "        soft_E = F.softmax(scaled_E)\n",
        "        attention = torch.matmul(soft_E , V)\n",
        "        result = FFN_O(attention)\n",
        "\n",
        "        return result "
      ],
      "metadata": {
        "id": "ufHJLO8qthoR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Cross_attention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.FFN_Q_Dec = nn.Linear(d_model,d_model)\n",
        "    self.FFN_K_Enc = nn.Linear(d_model,d_model)\n",
        "    self.FFN_V_Enc = nn.Linear(d_model,d_model)\n",
        "    self.FFN_Crs = nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,Encoder_output,Decoder_input):\n",
        "    \n",
        "    Q_Dec = FFN_Q(Decoder_input)\n",
        "    K_Enc = FFN_K(Encoder_output)\n",
        "    V_Enc = FFN_K(Encoder_output)\n",
        "\n",
        "    scaled_E = torch.matmul(Q,K.transpose(-2,1))/np.sqrt(d_k)\n",
        "    soft_E = F.softmax(scaled_E)\n",
        "    attention = torch.matmul(soft_E , V)\n",
        "    result = FFN_Crs(attention)\n",
        "    \n",
        "    return result"
      ],
      "metadata": {
        "id": "5eFuO4wPBavI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " class Layer_normal(nn.Module):\n",
        "\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input, MHA_output):\n",
        "    torch.layer_norm(input + MHA_output,[self.input[0],self.input[1],self.input[2]])    "
      ],
      "metadata": {
        "id": "48HSWZzgFHna"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Position_Wise_Feed_forward(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def _call__(self):\n",
        "\n",
        "    self.input_data = input_data\n",
        "    self.d_model = self.input_data.shape[1]\n",
        "    self.d_ff = self.d_model * 4\n",
        "    \n",
        "    self.first_layer = nn.Linear(self.d_model,self.d_ff)\n",
        "    self.second_layer = nn.Linear(self.d_ff,self.d_model)\n",
        "  \n",
        "  def forward(self, input_data):\n",
        "    hidden_state = self.first_layer(input_data)\n",
        "    output = self.second_layer(hidden_state)\n",
        "    return output"
      ],
      "metadata": {
        "id": "AxEmAKwA4UtF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "Eput = np.random.randn(10,10,10)\n",
        "np.random.seed(2)\n",
        "Dput = np.random.randn(10,10,10)\n",
        "trans = Transformer(Encoder(3) , Decoder(3))"
      ],
      "metadata": {
        "id": "SWv99vt3RLvu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(trans):,} trainable parameters')\n",
        "\n",
        "# 학습가능한 파라미터가 여러개 나왔어야하는데...아직 학습 부족으로 인해서 학습가능한 파라미터로 구성하는 방법에대해서는 잘 모르겠습니다ㅠㅠ\n",
        "# 계속 공부해나가면서 알아보겠습니다!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlNbOtyDeNmd",
        "outputId": "58cdb746-2d44-4e05-e35d-3c05a0fe9750"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 0 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans(Eput,Dput)"
      ],
      "metadata": {
        "id": "Gml1m2rregvI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FxqU7WhfVUCQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}