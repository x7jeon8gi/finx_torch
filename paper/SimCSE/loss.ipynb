{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cloning codes of Contrastive Learning\n",
    "# It will be linked with finx_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging # logging을 안쓰는 model이 없다. 사용법 꼭 숙지!\n",
    "# 간단히 말하면, 오류 발생시 출력해주는 놈!\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from utils import Metric # utils.py 구현해야함.\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances,paired_euclidean_distances,paired_manhattan_distances\n",
    "# 코사인 거리, 유클리디안 거리, 맨해튼 거리 -> 이거 어디서 쓰는 거였지...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger =  logging.getLogger(__name__)\n",
    "# 오류의 이름을 설정하고 \n",
    "# 이후에 오류의 레벨을 결정한다.\n",
    "# https://m.blog.naver.com/qvinci/221919758080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 16, 4]), 5, torch.Size([5]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity들의 계산의 결과를 살펴보자.\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "A = torch.randn([5,4,3])\n",
    "B = torch.randn([7,4,3])\n",
    "C = torch.randn([9,4,3])\n",
    "\n",
    "positive_similarity = nn.CosineSimilarity(dim=-1)(A.unsqueeze(1), B.unsqueeze(0))\n",
    "negative_similarity = nn.CosineSimilarity(dim=-1)(A.unsqueeze(1), C.unsqueeze(0))\n",
    "cosine_similarity = torch.cat([positive_similarity, negative_similarity], dim=1)\n",
    "\n",
    "positive_similarity.shape , negative_similarity.shape , cosine_similarity.shape\n",
    "\n",
    "labels = torch.arange(cosine_similarity.size(0)).long()\n",
    "\n",
    "cosine_similarity.shape ,cosine_similarity.size(0) , labels.shape\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss():\n",
    "\n",
    "    def __init__(self, args): #참고:args는 arguments를 뜻한다.\n",
    "        self.args = args\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "        # cos_sim 계산 시, 두 집단\n",
    "        self.metric = Metric(args)\n",
    "\n",
    "    def train_loss_fct(self,config,a,p,n):\n",
    "        \n",
    "        # 이 config는 자꾸 어디서 가져오는거지;;\n",
    "        # 본격적인 contrastive learning\n",
    "        \n",
    "        positive_similarity = self.cos(a.unsqueeze(1), p.unsqueeze(0)) / self.args.temperature\n",
    "        negative_similarity = self.cos(a.unsqueeze(1), n.unsqueeze(0)) / self.args.temperature\n",
    "        \n",
    "        # CosineSimilarity계산 시, 내적 계산이 들어가므로, \n",
    "        # 투입되는 두 요소가 하나는 row, 나머지 하나는 column\n",
    "        #\n",
    "        # 이 두 값은 이 후, 거리 계산 시에 자연 상수 e의 지수로 올라감.\n",
    "        #  \n",
    "        # a = h_i , p = h_i_+ , n = h_j\n",
    "        # a와 p는 서로 pair인 단어들의 hidden_states\n",
    "        # a와 n은 서로 pair가 아닌 단어들의 hidden_states\n",
    "        # \n",
    "        # self.args.temperature => temperature hyper_parameter \n",
    "        # temperature param에 대한 설명 => https://velog.io/@jkl133/temperature-parameter-in-learner-fastai\n",
    "        \n",
    "        cosine_similarity = torch.cat([positive_similarity, negative_similarity], dim=1).to(self.args.device)\n",
    "      \n",
    "        # 위의 cosine_similarity는 아직 뭘 위해서 정의한건지는 모르겠음.\n",
    "        # similarity들의 size는 a = (b1,r,c),p = (b2,r,c),n = (b3,r,c) 라 했을때\n",
    "        # positive_similarity = (b1,b2,r)\n",
    "        # negative_similarity = (b1,b3,r)\n",
    "        # 두개 concat = (b1,b2+b3,r)\n",
    "        # 단 r과 c는 셋 다 동일한 값을 가져야 한다.\n",
    "\n",
    "        labels = torch.arange(cosine_similarity.size(0)).long().to(self.args.device)\n",
    "\n",
    "        loss = config[\"criterion\"](cosine_similarity,labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_during_training(self, embeddings1, embeddings2, labels, indicator):\n",
    "\n",
    "        embeddings1 = embeddings1.cpu().numpy()\n",
    "        embeddings2 = embeddings2.cpu().numpy()\n",
    "        labels = labels.cpu().numpy().flatten()\n",
    "\n",
    "        cosine_scores = 1 - (paired_cosine_distances(embeddings1, embeddings2))\n",
    "        manhattan_distances = -paired_manhattan_distances(embeddings1, embeddings2)\n",
    "        euclidean_distances = -paired_euclidean_distances(embeddings1, embeddings2)\n",
    "        dot_products = [np.dot(emb1, emb2) for emb1, emb2 in zip(embeddings1, embeddings2)]\n",
    "\n",
    "        eval_pearson_cosine, _ = pearsonr(labels, cosine_scores)\n",
    "        eval_spearman_cosine, _ = spearmanr(labels, cosine_scores)\n",
    "\n",
    "        eval_pearson_manhattan, _ = pearsonr(labels, manhattan_distances)\n",
    "        eval_spearman_manhattan, _ = spearmanr(labels, manhattan_distances)\n",
    "\n",
    "        eval_pearson_euclidean, _ = pearsonr(labels, euclidean_distances)\n",
    "        eval_spearman_euclidean, _ = spearmanr(labels, euclidean_distances)\n",
    "\n",
    "        eval_pearson_dot, _ = pearsonr(labels, dot_products)\n",
    "        eval_spearman_dot, _ = spearmanr(labels, dot_products)\n",
    "\n",
    "        score = {'eval_pearson_cosine': eval_pearson_cosine,\n",
    "                 'eval_spearman_cosine': eval_spearman_cosine,\n",
    "                 'eval_pearson_manhattan': eval_pearson_manhattan,\n",
    "                 'eval_spearman_manhattan': eval_spearman_manhattan,\n",
    "                 'eval_pearson_euclidean': eval_pearson_euclidean,\n",
    "                 'eval_spearman_euclidean': eval_spearman_euclidean,\n",
    "                 'eval_pearson_dot': eval_pearson_dot,\n",
    "                 'eval_spearman_dot': eval_spearman_dot}\n",
    "\n",
    "        self.metric.update_indicator(indicator, score)\n",
    "\n",
    "        return max(eval_spearman_cosine, eval_spearman_manhattan, eval_spearman_euclidean, eval_spearman_dot)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
