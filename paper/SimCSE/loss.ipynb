{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cloning codes of Contrastive Learning\n",
    "# It will be linked with finx_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging # logging을 안쓰는 model이 없다. 사용법 꼭 숙지!\n",
    "# 간단히 말하면, 오류 발생시 출력해주는 놈!\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# from model.utils import Metric # utils.py 구현해야함.\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances,paired_euclidean_distances,paired_manhattan_distances\n",
    "# 코사인 거리, 유클리디안 거리, 맨해튼 거리 -> 이거 어디서 쓰는 거였지...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger =  logging.getLogger(__name__)\n",
    "# 오류의 이름을 설정하고 \n",
    "# 이후에 오류의 레벨을 결정한다.\n",
    "# https://m.blog.naver.com/qvinci/221919758080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8949, -0.5593, -1.8199],\n",
       "         [ 0.1003, -0.4994,  1.1032],\n",
       "         [ 0.4989, -0.9198, -1.0202]],\n",
       "\n",
       "        [[ 0.2898, -0.0192,  0.2035],\n",
       "         [-0.0123,  0.4207,  1.2705],\n",
       "         [ 0.1648, -0.7334, -0.1596]],\n",
       "\n",
       "        [[ 0.2317, -0.2079,  1.2720],\n",
       "         [ 1.5174, -0.5012, -0.1081],\n",
       "         [-0.2351,  0.2800, -1.5462]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 7, 4]), torch.Size([5, 9, 4]), torch.Size([5, 16, 4]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity들의 계산의 결과를 살펴보자.\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "A = torch.randn([5,4,3])\n",
    "B = torch.randn([7,4,3])\n",
    "C = torch.randn([9,4,3])\n",
    "\n",
    "positive_similarity = nn.CosineSimilarity(dim=-1)(A.unsqueeze(1), B.unsqueeze(0))\n",
    "negative_similarity = nn.CosineSimilarity(dim=-1)(A.unsqueeze(1), C.unsqueeze(0))\n",
    "cosine_similarity = torch.cat([positive_similarity, negative_similarity], dim=1)\n",
    "\n",
    "positive_similarity.shape , negative_similarity.shape , cosine_similarity.shape\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss():\n",
    "\n",
    "    def __init__(self, args): #참고:args는 arguments를 뜻한다.\n",
    "        self.args = args\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "        # cos_sim 계산 시, 두 집단\n",
    "        self.metric = Metric(args)\n",
    "\n",
    "    def train_loss_fct(self,config,a,p,n):\n",
    "        \n",
    "        # 이 config는 자꾸 어디서 가져오는거지;;\n",
    "        # 본격적인 contrastive learning\n",
    "        \n",
    "        positive_similarity = self.cos(a.unsqueeze(1), p.unsqueeze(0)) / self.args.temperature\n",
    "        negative_similarity = self.cos(a.unsqueeze(1), n.unsqueeze(0)) / self.args.temperature\n",
    "        \n",
    "        # CosineSimilarity계산 시, 내적 계산이 들어가므로, \n",
    "        # 투입되는 두 요소가 하나는 row, 나머지 하나는 column\n",
    "        #\n",
    "        # 이 두 값은 이 후, 거리 계산 시에 자연 상수 e의 지수로 올라감.\n",
    "        #  \n",
    "        # a = h_i , p = h_i_+ , n = h_j\n",
    "        # a와 p는 서로 pair인 단어들의 hidden_states\n",
    "        # a와 n은 서로 pair가 아닌 단어들의 hidden_states\n",
    "        # \n",
    "        # self.args.temperature => temperature hyper_parameter \n",
    "        # temperature param에 대한 설명 => https://velog.io/@jkl133/temperature-parameter-in-learner-fastai\n",
    "        \n",
    "        cosine_similarity = torch.cat([positive_similarity, negative_similarity], dim=1).to(self.args.device)\n",
    "      \n",
    "        # 위의 cosine_similarity는 아직 뭘 위해서 정의한건지는 모르겠음.\n",
    "        # similarity들의 size는 a = (b1,r,c),p = (b2,r,c),n = (b3,r,c) 라 했을때\n",
    "        # positive_similarity = (b1,b2,r)\n",
    "        # negative_similarity = (b1,b3,r)\n",
    "        # 두개 concat = (b1,b2+b3,r)\n",
    "        # 단 r과 c는 셋 다 동일한 값을 가져야 한다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
